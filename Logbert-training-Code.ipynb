{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#######################################################################################\n",
        "!pip install torch torchvision torchaudio --quiet\n",
        "!pip install numpy pandas scikit-learn tqdm matplotlib --quiet\n",
        "########################################################################################\n",
        "\n",
        "!git clone https://github.com/HelenGuohx/logbert.git\n",
        "%cd logbert\n",
        "\n",
        "\n",
        "!ls\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/logbert\")  # Add root folder to Python path\n",
        "\n",
        "\n",
        "import torch\n",
        "from bert_pytorch.model.log_model import BERTLog\n",
        "from bert_pytorch.model.bert import BERT\n",
        "\n",
        "\n",
        "vocab_size = 10000        # Total unique log keys in your dataset\n",
        "max_len = 128             # Sequence length\n",
        "hidden = 768              # Hidden size\n",
        "n_layers = 12             # Transformer blocks\n",
        "attn_heads = 12           # Attention heads\n",
        "dropout = 0.3\n",
        "is_logkey = True\n",
        "is_time = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "bert_model = BERT(\n",
        "    vocab_size=vocab_size,\n",
        "    max_len=max_len,\n",
        "    hidden=hidden,\n",
        "    n_layers=n_layers,\n",
        "    attn_heads=attn_heads,\n",
        "    dropout=dropout,\n",
        "    is_logkey=is_logkey,\n",
        "    is_time=is_time\n",
        ").to(device)\n",
        "\n",
        "\n",
        "model = BERTLog(bert_model, vocab_size=vocab_size).to(device)\n",
        "\n",
        "\n",
        "\n",
        "dummy_input = torch.randint(0, vocab_size, (2, max_len)).to(device)\n",
        "time_info = torch.zeros((2, max_len)).to(device)  # Required even if is_time=False\n",
        "\n",
        "output = model(dummy_input, time_info)\n",
        "\n",
        "print(\"Output type:\", type(output))\n",
        "if isinstance(output, torch.Tensor):\n",
        "    print(\"Output shape:\", output.shape)\n",
        "else:\n",
        "    print(\"Output:\", output)\n"
      ],
      "metadata": {
        "id": "f6TsLV66bKOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json, torch, random, os\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "train_file = \"/content/3.2_TRAIN.json\"\n",
        "test_file = \"/content/3.2_test.json\"\n",
        "max_len = 1000  # adjust as needed\n",
        "hidden = 240\n",
        "n_layers = 7\n",
        "attn_heads = 8\n",
        "dropout = 0.3\n",
        "batch_size = 64\n",
        "lr = 2e-3\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "output_dir = \"output3\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ==============================\n",
        "#  Build vocab\n",
        "# ==============================\n",
        "log2idx = {\"<PAD>\": 0}\n",
        "\n",
        "def build_vocab(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            item = json.loads(line)\n",
        "            for seq in item[\"numeric_sequence\"]:\n",
        "                for x in seq:\n",
        "                    if x not in log2idx:\n",
        "                        log2idx[x] = len(log2idx)\n",
        "\n",
        "# Build vocab from train + test\n",
        "build_vocab(train_file)\n",
        "build_vocab(test_file)\n",
        "vocab_size = len(log2idx)\n",
        "idx2log = {v:k for k,v in log2idx.items()}\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "\n",
        "# Save label mapping\n",
        "with open(os.path.join(output_dir, \"label_map.json\"), \"w\") as f:\n",
        "    json.dump(log2idx, f, indent=4)\n",
        "\n",
        "# ==============================\n",
        "# Dataset\n",
        "# ==============================\n",
        "class LazyLogDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        with open(file_path, \"r\") as f:\n",
        "            self.length = sum(1 for _ in f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        with open(self.file_path, \"r\") as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i == idx:\n",
        "                    item = json.loads(line)\n",
        "                    seqs = item[\"numeric_sequence\"][0]  # pick first sequence\n",
        "                    seqs = [log2idx[x] for x in seqs]\n",
        "                    # pad/truncate\n",
        "                    seqs += [log2idx[\"<PAD>\"]] * (max_len - len(seqs))\n",
        "                    seqs = seqs[:max_len]\n",
        "                    seq_tensor = torch.tensor(seqs, dtype=torch.long)\n",
        "                    time_tensor = torch.zeros(max_len, dtype=torch.float32)\n",
        "                    return seq_tensor, time_tensor, item[\"sequence_id\"]\n",
        "\n",
        "train_dataset = LazyLogDataset(train_file)\n",
        "test_dataset = LazyLogDataset(test_file)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# ==============================\n",
        "#  Model\n",
        "# ==============================\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, hidden, n_layers, attn_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, hidden, padding_idx=0)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden, nhead=attn_heads, dropout=dropout)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.max_len = max_len\n",
        "        self.hidden = hidden\n",
        "\n",
        "    def forward(self, x, t=None):\n",
        "        emb = self.emb(x) * (self.hidden ** 0.5)\n",
        "        emb = emb.permute(1, 0, 2)  # seq_len x batch x hidden\n",
        "        enc = self.encoder(emb)\n",
        "        enc = enc.permute(1, 0, 2)  # batch x seq_len x hidden\n",
        "        return enc\n",
        "\n",
        "class BERTLog(nn.Module):\n",
        "    def __init__(self, bert_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        self.fc = nn.Linear(bert_model.hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x, t=None):\n",
        "        enc = self.bert(x, t)\n",
        "        out = self.fc(enc)\n",
        "        return {\"logkey_output\": out}\n",
        "\n",
        "# ==============================\n",
        "#  Model, criterion, optimizer\n",
        "# ==============================\n",
        "bert_model = BERT(vocab_size=vocab_size, max_len=max_len, hidden=hidden,\n",
        "                  n_layers=n_layers, attn_heads=attn_heads, dropout=dropout).to(device)\n",
        "model = BERTLog(bert_model, vocab_size=vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# ==============================\n",
        "#  Training loop with metrics\n",
        "# ==============================\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for seqs, time_feats, _ in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
        "        seqs, time_feats = seqs.to(device), time_feats.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(seqs, time_feats)['logkey_output']\n",
        "        loss = criterion(outputs.view(-1, vocab_size), seqs.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for seqs, time_feats, _ in test_loader:\n",
        "            seqs, time_feats = seqs.to(device), time_feats.to(device)\n",
        "            outputs = model(seqs, time_feats)['logkey_output']\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "            val_preds.extend(preds.cpu().numpy().flatten())\n",
        "            val_labels.extend(seqs.cpu().numpy().flatten())\n",
        "\n",
        "    acc = accuracy_score(val_labels, val_preds) * 100\n",
        "    f1 = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
        "    precision = precision_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
        "    cm = confusion_matrix(val_labels, val_preds)\n",
        "    report = classification_report(val_labels, val_preds, zero_division=0)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}: Train Loss={avg_loss:.4f}, Val Acc={acc:.2f}%\")\n",
        "    print(f\"F1={f1:.4f}, Precision={precision:.4f}, Recall={recall:.4f}\")\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# ==============================\n",
        "# Save model\n",
        "# ==============================\n",
        "model_path = os.path.join(output_dir, \"model_final.pt\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved at {model_path}\")\n",
        "\n",
        "# ==============================\n",
        "#  Load model example\n",
        "# ==============================\n",
        "# To reload model for inference/testing/XAI/RAG\n",
        "loaded_model = BERTLog(BERT(vocab_size, max_len, hidden, n_layers, attn_heads, dropout), vocab_size).to(device)\n",
        "loaded_model.load_state_dict(torch.load(model_path))\n",
        "loaded_model.eval()\n",
        "print(\"Model loaded and ready for inference.\")\n"
      ],
      "metadata": {
        "id": "wkDquGOkbLX5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}